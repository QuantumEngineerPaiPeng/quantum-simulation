{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym_xy\n",
    "import gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from scipy.linalg import expm, logm\n",
    "import random\n",
    "from mpl_toolkits import mplot3d\n",
    "import scipy.integrate as integrate\n",
    "from sympy import lambdify, Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym.wrappers import Monitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Pauli(n):\n",
    "    if n==0:\n",
    "      return np.eye(2)\n",
    "    elif n==1:\n",
    "      return np.array([[0,1],[1,0]])\n",
    "    elif n==2:\n",
    "      return np.array([[0,-1j],[1j,0]])\n",
    "    elif n==3:\n",
    "      return np.array([[1,0],[0,-1]])\n",
    "    else:\n",
    "      raise ValueError('Input must be integer from 0 to 3.')\n",
    "\n",
    "# returns sigma_a^p*sigma_b^q, with a,b = 1,2,3, p,q being position\n",
    "def Kron2body(N_atom,a,b,p,q):\n",
    "    y=1\n",
    "    for i in range(N_atom):\n",
    "        if i==p:\n",
    "            y=np.kron(y,Pauli(a))\n",
    "        elif i==q:\n",
    "            y=np.kron(y,Pauli(b))\n",
    "        else:\n",
    "            y=np.kron(y,np.eye(2))\n",
    "    return y\n",
    "\n",
    "def Hamiltonian(N_atom,bc,cplist,model):\n",
    "    H=np.zeros((2**N_atom,2**N_atom))\n",
    "    for pp in range(len(cplist)):\n",
    "        for p in range(N_atom):\n",
    "            if bc=='p':\n",
    "                q=(p+pp+1)%N_atom\n",
    "            elif bc=='o':\n",
    "                q=p+pp+1\n",
    "                if q>=N_atom:\n",
    "                    continue\n",
    "            H=H+cplist[pp]*(model[0]*Kron2body(N_atom,1,1,p,q)\n",
    "                            +model[1]*Kron2body(N_atom,2,2,p,q)\n",
    "                            +model[2]*Kron2body(N_atom,3,3,p,q))+model[3]*Kron2body(N_atom,3,0,p,q)\n",
    "    if np.max(np.abs(np.imag(H)))<1e-10:                                         #why?\n",
    "        H=np.real(H)\n",
    "    return H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CartPoleAI(nn.Module):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "            self.fc = nn.Sequential(\n",
    "                        nn.Linear(13,64, bias=True),\n",
    "                        nn.ReLU(),\n",
    "                        nn.Linear(64,64, bias=True),\n",
    "                        nn.ReLU(),\n",
    "#                         nn.Linear(128,128, bias=True),\n",
    "#                         nn.ReLU(),\n",
    "#                         nn.Linear(64,64, bias=True),\n",
    "#                         nn.ReLU(),\n",
    "                        nn.Linear(64,5, bias=True),\n",
    "                        nn.Softmax(dim=1)\n",
    "                        )##### first one state_dim=(e.g.13) last one action_dim=(e.g.5)\n",
    "\n",
    "                \n",
    "        def forward(self, inputs):\n",
    "            x = self.fc(inputs)\n",
    "            return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(m):\n",
    "    \n",
    "        # nn.Conv2d weights are of shape [16, 1, 3, 3] i.e. # number of filters, 1, stride, stride\n",
    "        # nn.Conv2d bias is of shape [16] i.e. # number of filters\n",
    "        \n",
    "        # nn.Linear weights are of shape [32, 24336] i.e. # number of input features, number of output features\n",
    "        # nn.Linear bias is of shape [32] i.e. # number of output features\n",
    "        \n",
    "        if ((type(m) == nn.Linear) | (type(m) == nn.Conv2d)):\n",
    "            torch.nn.init.xavier_uniform(m.weight)\n",
    "            m.bias.data.fill_(0.00)\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_random_agents(num_agents):\n",
    "    \n",
    "    agents = []\n",
    "    for _ in range(num_agents):\n",
    "        \n",
    "        agent = CartPoleAI()\n",
    "        \n",
    "        for param in agent.parameters():\n",
    "            param.requires_grad = False\n",
    "            \n",
    "        init_weights(agent)\n",
    "        agents.append(agent)\n",
    "        \n",
    "        \n",
    "    return agents\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_agents(agents):\n",
    "    \n",
    "    reward_agents = []\n",
    "    env = gym.make('xy-v0')\n",
    "    \n",
    "    \n",
    "    maxTime=12\n",
    "    nSpin=3\n",
    "    min_delay=0\n",
    "    max_delay=1\n",
    "    pw=0.5\n",
    "    env.setParam(maxTime,nSpin,min_delay,max_delay,pw)\n",
    "\n",
    "\n",
    "\n",
    "    Aim=np.zeros([2**nSpin,2**nSpin])\n",
    "    env.setTargetH(Aim)\n",
    "\n",
    "    H=Hamiltonian(nSpin,'p',[1],[-0.5,-0.5,1,0])\n",
    "    J=8.18e-3\n",
    "    env.setH0(J*H)   \n",
    "    \n",
    "    \n",
    "    for agent in agents:\n",
    "        agent.eval()\n",
    "    \n",
    "        observation,info = env.reset()\n",
    "        \n",
    "        r=0\n",
    "        s=0\n",
    "        \n",
    "        for i in range(maxTime):\n",
    "            \n",
    "            inp = torch.tensor(observation).type('torch.FloatTensor').view(1,-1)\n",
    "#             print(inp)\n",
    "            output_probabilities = agent(inp).detach().numpy()[0]\n",
    "            action = np.random.choice(range(game_actions), 1, p=output_probabilities).item()\n",
    "            new_observation, reward, done, info = env.step(action,i)\n",
    "            r=r+reward\n",
    "            \n",
    "            s=s+1\n",
    "            observation = new_observation\n",
    "\n",
    "            if(done):\n",
    "                break\n",
    "\n",
    "        reward_agents.append(r)        \n",
    "        #reward_agents.append(s)\n",
    "        if r>16:\n",
    "            print(r)\n",
    "            print(observation)\n",
    "        \n",
    "    \n",
    "    return reward_agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_average_score(agent, runs):\n",
    "    score = 0.\n",
    "    for i in range(runs):\n",
    "        score += run_agents([agent])[0]\n",
    "    return score/runs\n",
    "\n",
    "#     score=0.\n",
    "#     for i in range(runs):\n",
    "#         temp=run_agents([agent])[0]\n",
    "#         if temp>score:\n",
    "#              score = temp\n",
    "#     return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_agents_n_times(agents, runs):\n",
    "    avg_score = []\n",
    "    for agent in agents:\n",
    "        avg_score.append(return_average_score(agent,runs))\n",
    "    return avg_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mutate(agent):\n",
    "\n",
    "    child_agent = copy.deepcopy(agent)\n",
    "    \n",
    "    mutation_power = 0.005#hyper-parameter, set from https://arxiv.org/pdf/1712.06567.pdf\n",
    "            \n",
    "    for param in child_agent.parameters():\n",
    "    \n",
    "        if(len(param.shape)==4): #weights of Conv2D\n",
    "\n",
    "            for i0 in range(param.shape[0]):\n",
    "                for i1 in range(param.shape[1]):\n",
    "                    for i2 in range(param.shape[2]):\n",
    "                        for i3 in range(param.shape[3]):\n",
    "                            \n",
    "                            param[i0][i1][i2][i3]+= mutation_power * np.random.randn()\n",
    "                                \n",
    "                                    \n",
    "\n",
    "        elif(len(param.shape)==2): #weights of linear layer\n",
    "            for i0 in range(param.shape[0]):\n",
    "                for i1 in range(param.shape[1]):\n",
    "                    \n",
    "                    param[i0][i1]+= mutation_power * np.random.randn()\n",
    "                        \n",
    "\n",
    "        elif(len(param.shape)==1): #biases of linear layer or conv layer\n",
    "            for i0 in range(param.shape[0]):\n",
    "                \n",
    "                param[i0]+=mutation_power * np.random.randn()\n",
    "\n",
    "    return child_agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_children(agents, sorted_parent_indexes, elite_index):\n",
    "    \n",
    "    children_agents = []\n",
    "    \n",
    "    #first take selected parents from sorted_parent_indexes and generate N-1 children\n",
    "    for i in range(len(agents)-1):\n",
    "        \n",
    "        selected_agent_index = sorted_parent_indexes[np.random.randint(len(sorted_parent_indexes))]\n",
    "        children_agents.append(mutate(agents[selected_agent_index]))\n",
    "\n",
    "    #now add one elite\n",
    "    elite_child, top_score = add_elite(agents, sorted_parent_indexes, elite_index)\n",
    "    children_agents.append(elite_child)\n",
    "    elite_index=len(children_agents)-1 #it is the last one\n",
    "    \n",
    "    return children_agents, elite_index, top_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_elite(agents, sorted_parent_indexes, elite_index=None, only_consider_top_n=10):\n",
    "    \n",
    "    candidate_elite_index = sorted_parent_indexes[:only_consider_top_n]\n",
    "    \n",
    "    if(elite_index is not None):\n",
    "        candidate_elite_index = np.append(candidate_elite_index,[elite_index])\n",
    "        \n",
    "    top_score = None\n",
    "    top_elite_index = None\n",
    "    \n",
    "    for i in candidate_elite_index:\n",
    "        score = return_average_score(agents[i],runs=5)\n",
    "        print(\"Score for elite i \", i, \" is \", score)\n",
    "        \n",
    "        if(top_score is None):\n",
    "            top_score = score\n",
    "            top_elite_index = i\n",
    "        elif(score > top_score):\n",
    "            top_score = score\n",
    "            top_elite_index = i\n",
    "            \n",
    "    print(\"Elite selected with index \",top_elite_index, \" and score\", top_score)\n",
    "    \n",
    "    child_agent = copy.deepcopy(agents[top_elite_index])\n",
    "    return child_agent, top_score\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "    return np.exp(x) / np.sum(np.exp(x), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def selection(n,rewards,topn):\n",
    "#     t0=1\n",
    "#     tf=1e-4\n",
    "#     yita=0.1\n",
    "#     lamb=1\n",
    "#     nc=21\n",
    "#     t=t0*(tf/t0)**(n/nc)*(1-yita*np.sin(lamb*np.pi*n/nc))\n",
    "#     qbest=np.amax(rewards)\n",
    "#     q=np.exp((rewards-qbest)/t)\n",
    "#     prob=q/np.sum(q)\n",
    "# #     print(list(enumerate(rewards)))\n",
    "          \n",
    "#     return np.random.choice(len(rewards),topn,p=prob)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19.41975673669137\n",
      "[ 0.75  1.    1.    1.    1.    0.75  1.    1.    1.    1.    0.25  0.25\n",
      " 13.  ]\n"
     ]
    }
   ],
   "source": [
    "game_actions = 5 #2 actions possible: left or right\n",
    "\n",
    "#disable gradients as we will not use them\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "# initialize N number of agents\n",
    "num_agents = 2000\n",
    "agents = return_random_agents(num_agents)\n",
    "\n",
    "# How many top agents to consider as parents\n",
    "top_limit = 20\n",
    "\n",
    "# run evolution until X generations\n",
    "generations = 100\n",
    "\n",
    "elite_index = None\n",
    "\n",
    "mean_reward=[]\n",
    "mean_top5_reward=[]\n",
    "top_reward=[]\n",
    "elite_reward=[]\n",
    "\n",
    "for generation in range(generations):\n",
    "#     print(generation)\n",
    "\n",
    "    # return rewards of agents\n",
    "    rewards = run_agents_n_times(agents, 1) #return average of 3 runs\n",
    "#     print(rewards)\n",
    "#     print(\"\")\n",
    "\n",
    "    # sort by rewards\n",
    "#     print(np.argsort(rewards)[::-1][:top_limit])\n",
    "    sorted_parent_indexes = np.argsort(rewards)[::-1][:top_limit]\n",
    "#     sorted_parent_indexes = selection (generation,rewards,top_limit)\n",
    "#     print(sorted_parent_indexes)\n",
    "    #reverses and gives top values (argsort sorts by ascending by default) https://stackoverflow.com/questions/16486252/is-it-possible-to-use-argsort-in-descending-order\n",
    "    print(\"\")\n",
    "    print(\"\")\n",
    "    \n",
    "    top_rewards = []\n",
    "    for best_parent in sorted_parent_indexes:\n",
    "        top_rewards.append(rewards[best_parent])\n",
    "    \n",
    "    print(\"Generation \", generation, \" | Mean rewards: \", np.mean(rewards), \" | Mean of top 5: \",np.mean(top_rewards[:5]))\n",
    "    #print(rewards)\n",
    "    print(\"Top \",top_limit,\" scores\", sorted_parent_indexes)\n",
    "    print(\"Rewards for top: \",top_rewards)\n",
    "    mean_reward.append(np.mean(rewards))\n",
    "    mean_top5_reward.append(np.mean(top_rewards[:5]))\n",
    "    top_reward.append(top_rewards[0])\n",
    "#     if top_rewards[0]>27:\n",
    "#         break\n",
    "    \n",
    "    # setup an empty list for containing children agents\n",
    "    children_agents, elite_index, top_score = return_children(agents, sorted_parent_indexes, elite_index)\n",
    "    elite_reward.append(top_score)\n",
    "    # kill all agents, and replace them with their children\n",
    "    agents = children_agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('GA-2layer-64.csv',np.array([mean_reward,mean_top5_reward,top_reward,elite_reward]).T,fmt=['%.7f','%.7f','%.7f','%.7f'],delimiter=',',header=\"mean_reward,mean_top5_reward,top_reward,elite_reward\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, ax = plt.subplots(figsize=(5,4),tight_layout=True)\n",
    "fig, ax = plt.subplots(tight_layout=True)\n",
    "\n",
    "ax.plot(mean_reward)\n",
    "ax.plot(mean_top5_reward)\n",
    "ax.plot(top_reward)\n",
    "ax.plot(elite_reward)\n",
    "\n",
    "\n",
    "# ax.set_xscale('log')\n",
    "\n",
    "ax.set(xlabel='Generation', ylabel='Reward',\n",
    "       title='Jt=8.18e-3')        ### The 12 pulse sequence is compared with the 6 pulse sequence\n",
    "                                   ### We regard the 6 pulse sequence is T long, the 12 pulse sequence is 2T long \n",
    "\n",
    "plt.legend(['mean_reward', 'mean_top5_reward','top_reward', 'elite_reward'], loc='best')\n",
    "\n",
    "plt.savefig('GA-2layer-64.eps', dpi=fig.dpi, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_agent(agent):\n",
    "#     try: #try and exception block because, render hangs if an erorr occurs, we must do env.close to continue working    \n",
    "        env = gym.make('xy-v0')\n",
    "\n",
    "\n",
    "        maxTime=12\n",
    "        nSpin=3\n",
    "        min_delay=0\n",
    "        max_delay=1\n",
    "        pw=0.0\n",
    "        env.setParam(maxTime,nSpin,min_delay,max_delay,pw)\n",
    "\n",
    "\n",
    "\n",
    "        Aim=np.zeros([2**nSpin,2**nSpin])\n",
    "        env.setTargetH(Aim)\n",
    "\n",
    "        H=Hamiltonian(nSpin,'p',[1],[-0.5,-0.5,1,0])\n",
    "        J=8.18e-3\n",
    "        env.setH0(J*H)  \n",
    "        \n",
    "        observation,info = env.reset()\n",
    "        last_observation = observation\n",
    "        r=0\n",
    "        for i in range(12):\n",
    "#             env_record.render()\n",
    "            inp = torch.tensor(observation).type('torch.FloatTensor').view(1,-1)\n",
    "            output_probabilities = agent(inp).detach().numpy()[0]\n",
    "            action = np.random.choice(range(game_actions), 1, p=output_probabilities).item()\n",
    "            print(action)\n",
    "            new_observation, reward, done, info = env.step(action,i)\n",
    "            r=r+reward\n",
    "            observation = new_observation\n",
    "\n",
    "            if(done):\n",
    "                break\n",
    "\n",
    "#         env_record.close()\n",
    "        print(\"Rewards: \",r)\n",
    "        print(observation)\n",
    "\n",
    "#     except Exception as e:\n",
    "# #         env_record.close()\n",
    "#         print(e.__doc__)\n",
    "#         print(e.message)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "play_agent(agents[406])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_agent():\n",
    "#     try: #try and exception block because, render hangs if an erorr occurs, we must do env.close to continue working    \n",
    "        env = gym.make('xy-v0')\n",
    "\n",
    "\n",
    "        maxTime=12\n",
    "        nSpin=3\n",
    "        min_delay=0\n",
    "        max_delay=1\n",
    "        pw=0.5\n",
    "        env.setParam(maxTime,nSpin,min_delay,max_delay,pw)\n",
    "\n",
    "\n",
    "\n",
    "        Aim=np.zeros([2**nSpin,2**nSpin])\n",
    "        env.setTargetH(Aim)\n",
    "\n",
    "        H=Hamiltonian(nSpin,'p',[1],[-0.5,-0.5,1,0])\n",
    "        J=8.18e-3\n",
    "        env.setH0(J*H)  \n",
    "        \n",
    "        observation,info = env.reset()\n",
    "        last_observation = observation\n",
    "        r=0\n",
    "    \n",
    "        \n",
    "        new_observation, reward, done, info = env.step(0,0)\n",
    "        new_observation, reward, done, info = env.step(1,1)\n",
    "        new_observation, reward, done, info = env.step(4,2)\n",
    "        new_observation, reward, done, info = env.step(0,3)\n",
    "        new_observation, reward, done, info = env.step(2,4)\n",
    "        new_observation, reward, done, info = env.step(3,5)\n",
    "        new_observation, reward, done, info = env.step(0,6)\n",
    "        new_observation, reward, done, info = env.step(3,7)\n",
    "        new_observation, reward, done, info = env.step(4,8)\n",
    "        new_observation, reward, done, info = env.step(0,9)\n",
    "        new_observation, reward, done, info = env.step(2,10)\n",
    "        new_observation, reward, done, info = env.step(1,11)\n",
    "        r=r+reward\n",
    "#         H1=env.getAHT1()\n",
    "#         H2=env.getAHT2()\n",
    "#         H3=env.getAHT3()\n",
    "# #         H4=env.getAHT4()\n",
    "# #         print(np.trace(np.transpose(np.conjugate(H1))*H1))\n",
    "# #         print(np.trace(np.transpose(np.conjugate(H2))*H2))\n",
    "# #         print(np.trace(np.transpose(np.conjugate(H3))*H3))\n",
    "# #         print(np.trace(np.transpose(np.conjugate(H4))*H4))\n",
    "#         print(-np.log(1-np.trace(expm(-1j*H1))/8))\n",
    "#         print(-np.log(1-np.trace(expm(-1j*(H1+H2)))/8))\n",
    "#         print(-np.log(1-np.trace(expm(-1j*(H1+H2+H3)))/8))\n",
    "#         print('fidelity')\n",
    "#         print(np.abs(np.sum(H1*np.transpose(np.conjugate(self.target)))/2**self.nSpin))\n",
    "\n",
    "#         if(done):\n",
    "#             break\n",
    "\n",
    "        print(\"Rewards: \",r)\n",
    "        print(new_observation)\n",
    "\n",
    "#     except Exception as e:\n",
    "# #         env_record.close()\n",
    "#         print(e.__doc__)\n",
    "#         print(e.message)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_agent()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
