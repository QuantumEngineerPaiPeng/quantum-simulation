{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement learning for Hamiltonian engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "\n",
    "import gym_pp\n",
    "import import_ipynb, time\n",
    "# import time\n",
    "# from RL_funcs import Hamiltonian, Q_table_ML, getAction_beta, enumerate_all\n",
    "import gym\n",
    "from scipy.linalg import expm\n",
    "from scipy import sparse\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": [
     23,
     33
    ]
   },
   "outputs": [],
   "source": [
    "import os, subprocess, signal\n",
    "from copy import deepcopy as dcp\n",
    "import gym\n",
    "from gym import error, spaces\n",
    "from gym import utils\n",
    "from gym.utils import seeding\n",
    "import numpy as np\n",
    "#from scipy.linalg import expm\n",
    "from scipy import sparse\n",
    "import random\n",
    "\n",
    "import gym_pp\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "# plt.rc('text', usetex=True)\n",
    "# font = {'family' : 'normal',\n",
    "#  'weight' : 'bold',\n",
    "#  'size' : 50}\n",
    "# plt.rc('font', **font)\n",
    "# from jupyterthemes import jtplot\n",
    "# jtplot.style()\n",
    "\n",
    "# 10-base to arbitrary base\n",
    "def dec2base(num,base,length):\n",
    "    s=''\n",
    "    if num>base**length-1:\n",
    "        raise ValueError('Input number exceeds the maximum number allowed by the length')\n",
    "    for i in range(length):\n",
    "        s=s+chr(ord('0')+int(num/(base**(length-1-i))))\n",
    "        num=num-int(num/(base**(length-1-i)))*(base**(length-1-i))\n",
    "    return s\n",
    "\n",
    "# given a state dict, return a str that specifies all pulses\n",
    "def getPPstr(state,actionDict=['d','x','y','-x','-y']):\n",
    "    num=state['pp']\n",
    "    base=len(actionDict)\n",
    "    length=state['n']\n",
    "    rawPulseStr=dec2base(num,base,length)\n",
    "    pulseStr=''\n",
    "    for p in rawPulseStr:\n",
    "        pulseStr=actionDict[int(p)]+','+pulseStr\n",
    "    pulseStr=pulseStr[0:-1]\n",
    "    return pulseStr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Pauli(n):\n",
    "    if n==0:\n",
    "      return np.eye(2)\n",
    "    elif n==1:\n",
    "      return np.array([[0,1],[1,0]])\n",
    "    elif n==2:\n",
    "      return np.array([[0,-1j],[1j,0]])\n",
    "    elif n==3:\n",
    "      return np.array([[1,0],[0,-1]])\n",
    "    else:\n",
    "      raise ValueError('Input must be integer from 0 to 3.')\n",
    "\n",
    "# returns sigma_a^p*sigma_b^q, with a,b = 1,2,3, p,q being position\n",
    "def Kron2body(N_atom,a,b,p,q):\n",
    "    y=1\n",
    "    for i in range(N_atom):\n",
    "        if i==p:\n",
    "            y=np.kron(y,Pauli(a))\n",
    "        elif i==q:\n",
    "            y=np.kron(y,Pauli(b))\n",
    "        else:\n",
    "            y=np.kron(y,np.eye(2))\n",
    "    return y\n",
    "\n",
    "def Hamiltonian(N_atom,bc,cplist,model):\n",
    "    H=np.zeros((2**N_atom,2**N_atom))\n",
    "    for pp in range(len(cplist)):\n",
    "        for p in range(N_atom):\n",
    "            if bc=='p':\n",
    "                q=(p+pp+1)%N_atom\n",
    "            elif bc=='o':\n",
    "                q=p+pp+1\n",
    "                if q>=N_atom:\n",
    "                    continue\n",
    "            H=H+cplist[pp]*(model[0]*Kron2body(N_atom,1,1,p,q)\n",
    "                            +model[1]*Kron2body(N_atom,2,2,p,q)\n",
    "                            +model[2]*Kron2body(N_atom,3,3,p,q))\n",
    "    if np.max(np.abs(np.imag(H)))<1e-10:\n",
    "        H=np.real(H)\n",
    "    return H\n",
    "\n",
    "# how many actions are available (maxTime is not considered)\n",
    "def getAvailableAction(state,frame):\n",
    "    if np.max(np.abs(frame-np.eye(2)))<1e-10 and state['n']!=0:\n",
    "        return 6\n",
    "    else:\n",
    "        return 5\n",
    "\n",
    "# get the next action by inverseT\n",
    "def getAction_beta(state,frame,beta, maxTime, qTable):\n",
    "    if state['n']==maxTime:\n",
    "        return 5\n",
    "    nAction= getAvailableAction(state,frame)\n",
    "    temp = qTable[state['pp'] + (state['n']-1) * 5** maxTime ,0:nAction]\n",
    "    if sparse.issparse(qTable):\n",
    "        temp = temp.A\n",
    "    prob=np.exp(beta*temp)\n",
    "    prob=prob/np.sum(prob)\n",
    "    prob = prob[0] if sparse.issparse(qTable) else prob\n",
    "    action=np.random.choice(nAction,1,p=prob)\n",
    "    return action[0]\n",
    "\n",
    "# get the next action by epsilon greedy\n",
    "def getAction_epsilon(state,frame,epsilon):\n",
    "    if state['n']==maxTime:\n",
    "        return 5\n",
    "    else:\n",
    "        nAction=getAvailableAction(state,frame)\n",
    "        if np.random.rand() < epsilon[episode]:\n",
    "            action = np.random.randint(0, nAction)\n",
    "        else:\n",
    "            temp = qTable[state['pp'], state['n'],:nAction]\n",
    "            action = np.where(abs(temp-max(temp))<1e-10 )\n",
    "            action = random.sample( list(action[0]) , 1)[0]\n",
    "        return action\n",
    "\n",
    "def Q_table_ML(env, qTable, nEpisodes, alpha_list, gamma, beta): # return reward_lsit, pulse_list, best_pp\n",
    "    maxTime = env.maxTime\n",
    "    best_reward=0\n",
    "    best_pp=None\n",
    "    reward_list=[]\n",
    "    reward_real_list=[]\n",
    "    pulse_list = []\n",
    "    best_list = []\n",
    "    for episode in range(nEpisodes):\n",
    "        episode_experiences = []\n",
    "        state,info= dcp( env.reset() )\n",
    "        frame=info[\"frame\"]\n",
    "        done=False\n",
    "        while not done:\n",
    "            action=getAction_beta(state,frame,beta[episode], maxTime, qTable)\n",
    "\n",
    "            next_state, reward, done, info, reward_real= dcp( env.step(action) )\n",
    "            episode_experiences.append( [ state, action, reward, info, next_state ] )\n",
    "            frame=info[\"frame\"]\n",
    "            alpha = alpha_list[episode]\n",
    "            if done:\n",
    "                state_index = state['pp'] + (state['n']-1) * 5** maxTime\n",
    "                qTable[state_index,action]=(1-alpha)*qTable[state_index,action]+alpha*reward\n",
    "                for step in range( state['n']-1, -1, -1):\n",
    "                    state_, action_, reward_, info_, next_state_ = episode_experiences[step]\n",
    "                    state_index = state_['pp'] + (state_['n']-1) * 5** maxTime\n",
    "                    next_state_index = next_state_['pp'] + (next_state_['n']-1) * 5** maxTime\n",
    "                    temp = qTable[next_state_index, :]\n",
    "                    if sparse.issparse(qTable):\n",
    "                        temp = temp.A\n",
    "                    qTable[state_index,action_]=((1-alpha)*qTable[state_index,action_] \\\n",
    "                                                                +alpha*(reward_+gamma*np.max(temp )))\n",
    "\n",
    "            else:\n",
    "                state=next_state\n",
    "        '''\n",
    "        if reward>best_reward:\n",
    "            best_reward=reward\n",
    "            best_pp=getPPstr(env.state)\n",
    "            best_list.append([episode, reward, best_pp ])\n",
    "            print('Episode: '+str(episode))\n",
    "            print(getPPstr(env.state))\n",
    "            print('Fidelity: '+str(env.getFidelity())+', Reward: '+str(reward))\n",
    "        if (episode+1)%10000==0 and episode!=0:\n",
    "            recent_reward=reward_list[episode-999:episode+1]\n",
    "            print('Recent 1000 average reward: '+str(np.mean(recent_reward)))\n",
    "        '''\n",
    "        reward_list.append(reward)\n",
    "        reward_real_list.append(reward_real)\n",
    "        pulse_list.append(getPPstr(env.state))\n",
    "#         pulse_list.append(env.state)\n",
    "        \n",
    "        if reward>28.6:\n",
    "            print(reward)\n",
    "            print(getPPstr(env.state))\n",
    "            print('Episode: '+str(episode))\n",
    "            break\n",
    "        \n",
    "    return reward_list, pulse_list, best_pp, reward_real_list\n",
    "\n",
    "# enumerate all pulses with maxTime, then sort by reward\n",
    "def enumerate_all(env, maxTime=6):\n",
    "    reward_all = []\n",
    "    for episode in range(5**maxTime):\n",
    "        pulse_num = episode\n",
    "        env.reset()\n",
    "        for step_index in range(maxTime):\n",
    "            observation, reward, done, info = env.step( pulse_num % 5 )\n",
    "            pulse_num //= 5\n",
    "            frame=info[\"frame\"]\n",
    "        observation, reward, done, info = env.step(5)\n",
    "        reward_all.append(reward)\n",
    "    return reward_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxTime=12\n",
    "nSpin=3\n",
    "pw=0.0\n",
    "\n",
    "env = gym.make('pp-v0')\n",
    "\n",
    "observation = env.reset()\n",
    "env.setParam(maxTime,nSpin,pw)\n",
    "\n",
    "H=Hamiltonian(nSpin,'p',[1],[-0.5,-0.5,1])\n",
    "J= 8.18e-3\n",
    "t=1\n",
    "env.setU0(expm(-1j*J*H*t))\n",
    "env.setH0(J*H)   \n",
    "\n",
    "H_ising=Hamiltonian(nSpin,'p',[1],[0, 0, 1])\n",
    "t_ising = 0\n",
    "env.setTarget(expm(-1j*J*H_ising*t_ising*maxTime), false_frame=-10)\n",
    "#env.setTarget(np.eye(2**nSpin), false_frame=-10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "code_folding": [
     0,
     9
    ],
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:1: DeprecationWarning: time.clock has been deprecated in Python 3.3 and will be removed from Python 3.8: use time.perf_counter or time.process_time instead\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beta!=0, 1st reward>23 episode=\n",
      "28.6026262975392\n",
      "-y,-y,-x,-x,-y,x,-x,x,y,-y,x,y\n",
      "Episode: 1870\n",
      "28.60233299939954\n",
      "-x,d,x,y,-y,-y,-x,y,y,-x,d,-y\n",
      "Episode: 4920\n",
      "28.602919681727894\n",
      "-y,-x,-x,-x,-y,y,x,y,y,y,-x,-x\n",
      "Episode: 11783\n",
      "28.603506708454468\n",
      "y,y,-y,-x,-x,-y,x,-x,-x,y,y,x\n",
      "Episode: 10808\n",
      "28.60233299939954\n",
      "-y,-y,-x,x,-y,x,x,-y,-x,y,y,x\n",
      "Episode: 3958\n",
      "28.602919681727894\n",
      "y,y,x,-x,x,-y,-x,x,-y,-y,y,x\n",
      "Episode: 10200\n",
      "28.60057501476864\n",
      "-y,-y,-x,-y,y,y,-x,y,-y,-x,-y,x\n",
      "Episode: 7326\n",
      "28.601453620770393\n",
      "-x,y,-y,y,x,y,y,-x,d,-y,-x,d\n",
      "Episode: 17821\n",
      "28.604387895176195\n",
      "-y,y,x,-x,y,x,x,-x,y,-y,-x,-y\n",
      "Episode: 204\n",
      "28.601746661065533\n",
      "-x,y,-x,-x,y,x,x,y,y,-x,-y,-y\n",
      "Episode: 12521\n",
      "28.602039787258455\n",
      "y,-y,-x,x,-y,-x,x,-y,x,y,y,x\n",
      "Episode: 6543\n",
      "28.60233299939954\n",
      "-x,x,y,d,-y,x,d,y,x,-x,-y,-x\n",
      "Episode: 8882\n",
      "28.602919681727894\n",
      "x,x,-x,y,-x,x,y,y,-y,x,-y,-y\n",
      "Episode: 8966\n",
      "28.604681796721295\n",
      "x,-x,x,-y,-x,-x,-y,-y,-y,-x,y,y\n",
      "Episode: 957\n",
      "28.601453620770393\n",
      "y,d,-x,-x,y,x,x,d,x,-y,y,-x\n",
      "Episode: 17261\n",
      "28.600867797672212\n",
      "-x,-y,y,y,x,-y,y,-x,-y,x,y,-y\n",
      "Episode: 1034\n",
      "28.601453620770393\n",
      "-y,x,-x,-x,y,d,-y,x,d,y,x,-x\n",
      "Episode: 2385\n",
      "28.60028231756181\n",
      "-y,x,-y,y,-x,d,y,x,y,-y,-x,d\n",
      "Episode: 12534\n",
      "28.601160666322716\n",
      "-x,-x,-y,x,-x,x,y,d,-y,x,d,-y\n",
      "Episode: 936\n",
      "28.603506708454468\n",
      "y,-x,y,-y,-y,x,x,-y,-x,x,-x,-y\n",
      "Episode: 2911\n",
      "28.602039787258455\n",
      "x,x,-y,-x,x,y,x,y,y,-x,y,y\n",
      "Episode: 2409\n",
      "28.60028231756181\n",
      "-x,x,-y,-x,d,y,-y,x,-y,y,d,y\n",
      "Episode: 4202\n",
      "28.602039787258455\n",
      "-x,-y,y,x,-x,y,-x,-x,-y,x,-y,-y\n",
      "Episode: 1004\n",
      "28.60380035109349\n",
      "-y,x,-x,-y,y,-y,x,-y,-y,-x,-x,-x\n",
      "Episode: 1183\n",
      "28.601160666322716\n",
      "y,x,y,y,x,-y,x,-y,-y,x,-y,y\n",
      "Episode: 3271\n",
      "28.601746661065533\n",
      "y,x,-x,-y,-x,-x,y,y,-x,-y,-y,x\n",
      "Episode: 10614\n",
      "28.602039787258455\n",
      "-y,-y,-x,y,y,d,-y,x,-x,y,-x,d\n",
      "Episode: 1749\n",
      "28.6026262975392\n",
      "y,x,x,-y,d,x,-y,d,-y,x,x,x\n",
      "Episode: 14857\n",
      "28.604094079983838\n",
      "x,-y,-y,x,y,-y,y,-x,x,y,-x,x\n",
      "Episode: 18440\n",
      "28.602919681727894\n",
      "-y,-y,x,-y,-y,-y,-x,x,y,-x,-x,-x\n",
      "Episode: 4581\n",
      "28.603213152016135\n",
      "-x,x,-y,y,-x,-y,y,y,x,-x,-y,x\n",
      "Episode: 19610\n",
      "28.601160666322716\n",
      "y,x,y,y,x,-y,x,x,y,-x,-x,-y\n",
      "Episode: 10217\n",
      "28.60233299939954\n",
      "x,-y,y,-x,x,x,-y,-x,-x,-y,y,-y\n",
      "Episode: 14720\n",
      "28.602039787258455\n",
      "y,y,y,-x,-y,-y,-x,d,-y,x,d,-x\n",
      "Episode: 3389\n",
      "28.601160666322716\n",
      "y,x,-y,y,-x,x,y,-x,-x,y,x,-y\n",
      "Episode: 18012\n",
      "28.600867797672212\n",
      "y,x,-x,-y,-x,-x,-y,-y,-x,y,y,x\n",
      "Episode: 2286\n",
      "28.601453620770393\n",
      "-x,x,-y,x,-x,-y,x,-y,y,-x,y,y\n",
      "Episode: 1377\n",
      "28.601453620770393\n",
      "d,-x,y,-y,y,x,-y,y,-x,d,-y,x\n",
      "Episode: 12669\n",
      "28.60057501476864\n",
      "-x,-y,y,-x,-y,d,x,-y,y,x,-y,d\n",
      "Episode: 5072\n",
      "28.60380035109349\n",
      "-x,y,-y,-y,-x,-y,y,x,x,x,-y,-x\n",
      "Episode: 502\n",
      "28.60028231756181\n",
      "x,y,y,-y,-x,-y,y,x,-y,-x,y,-y\n",
      "Episode: 4077\n",
      "28.60233299939954\n",
      "d,y,-y,-x,-y,y,d,x,-y,x,-x,y\n",
      "Episode: 3055\n",
      "28.601746661065533\n",
      "y,-x,y,y,x,-y,x,-x,y,x,x,y\n",
      "Episode: 14115\n",
      "28.603213152016135\n",
      "-y,-y,-x,x,-y,-x,-x,-x,-y,-y,x,y\n",
      "Episode: 14125\n",
      "28.60233299939954\n",
      "x,-x,y,x,-x,-y,x,y,y,x,-y,-y\n",
      "Episode: 28682\n",
      "28.60233299939954\n",
      "x,-x,y,-y,-x,-y,y,-x,y,x,x,y\n",
      "Episode: 2218\n",
      "28.601160666322716\n",
      "-y,y,x,y,y,x,x,-y,x,x,y,-x\n",
      "Episode: 1830\n",
      "28.601453620770393\n",
      "x,-y,-x,-x,d,y,-x,-y,-y,x,d,-x\n",
      "Episode: 11460\n",
      "28.602919681727894\n",
      "-y,x,x,-y,-x,-x,-y,-x,-y,y,x,y\n",
      "Episode: 68\n",
      "28.601160666322716\n",
      "-y,-x,y,x,-x,y,-x,-x,y,-y,x,-y\n",
      "Episode: 573\n",
      "28.601746661065533\n",
      "-y,x,-x,y,x,x,y,x,-y,-y,-x,-y\n",
      "Episode: 8091\n",
      "28.601746661065533\n",
      "x,-y,y,-x,y,-y,x,x,-y,-x,-x,-y\n",
      "Episode: 1889\n",
      "28.604094079983838\n",
      "-x,-y,-y,x,-x,-x,y,x,-x,-y,y,y\n",
      "Episode: 2657\n",
      "28.60380035109349\n",
      "x,x,y,-y,y,-x,-y,y,x,-x,-x,y\n",
      "Episode: 6024\n",
      "28.60028231756181\n",
      "-x,-y,y,x,-y,-x,y,-y,x,y,-y,y\n",
      "Episode: 7517\n",
      "28.604094079983838\n",
      "-x,-x,-y,x,x,x,y,-y,-x,y,-y,-y\n",
      "Episode: 4992\n",
      "28.601746661065533\n",
      "-y,x,x,y,y,-x,y,y,x,y,-x,x\n",
      "Episode: 1659\n",
      "28.60233299939954\n",
      "x,-y,-x,-x,x,y,-y,x,y,y,-y,-x\n",
      "Episode: 12101\n",
      "28.602039787258455\n",
      "-x,d,y,-x,d,-x,y,y,y,x,y,y\n",
      "Episode: 19203\n",
      "28.60380035109349\n",
      "-y,-x,x,y,-x,x,x,y,y,x,-y,-y\n",
      "Episode: 3305\n",
      "28.604681796721295\n",
      "-y,x,x,x,-y,-x,-x,y,-y,y,x,-y\n",
      "Episode: 1419\n",
      "28.60028231756181\n",
      "y,x,y,-x,y,-y,x,-y,y,-y,-x,-y\n",
      "Episode: 685\n",
      "28.602039787258455\n",
      "-y,x,d,x,-y,y,-y,-x,-y,y,-x,d\n",
      "Episode: 1407\n",
      "28.60233299939954\n",
      "-y,-y,x,y,-x,-x,y,x,x,y,y,x\n",
      "Episode: 9081\n",
      "28.601453620770393\n",
      "-x,y,-y,x,y,-y,-x,-y,-x,x,y,x\n",
      "Episode: 3343\n",
      "28.600867797672212\n",
      "x,-y,-x,-x,-y,d,x,-y,d,y,-x,x\n",
      "Episode: 777\n",
      "28.602039787258455\n",
      "-x,y,y,x,-y,-y,x,-x,y,x,x,y\n",
      "Episode: 9075\n",
      "28.603213152016135\n",
      "y,-y,-y,x,x,-y,-x,-x,x,-y,y,-x\n",
      "Episode: 7528\n",
      "28.601453620770393\n",
      "-y,-x,y,-y,-x,-y,-y,x,x,-y,x,-x\n",
      "Episode: 2303\n",
      "28.600867797672212\n",
      "d,x,-y,-x,x,y,d,y,y,x,-y,-y\n",
      "Episode: 3632\n",
      "28.6026262975392\n",
      "-x,-y,-x,-x,-x,y,y,-x,y,-y,y,-x\n",
      "Episode: 4498\n",
      "28.603506708454468\n",
      "-x,-x,x,y,y,-x,y,-y,-y,x,-x,-y\n",
      "Episode: 239\n",
      "28.601453620770393\n",
      "-y,x,x,y,x,x,-y,-y,-x,y,-y,x\n",
      "Episode: 6790\n",
      "28.60057501476864\n",
      "-y,-x,-y,-y,-x,y,-y,y,x,y,y,x\n",
      "Episode: 5251\n",
      "mean of episode= 7303.026666666667\n",
      "learning time=9782.577279000001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:30: DeprecationWarning: time.clock has been deprecated in Python 3.3 and will be removed from Python 3.8: use time.perf_counter or time.process_time instead\n"
     ]
    }
   ],
   "source": [
    "start_t = time.clock()\n",
    "\n",
    "nEpisodes=int(5e4)\n",
    "\n",
    "alpha_list=np.linspace(0.8, 0.2, num=nEpisodes)# gradient step\n",
    "gamma=0.9 # discount factor\n",
    "# epsilon = np.linspace(0.5, 0.05, num=nEpisodes)\n",
    "#beta = np.zeros(nEpisodes)\n",
    "beta=np.logspace(-3.0, 1, num=nEpisodes) # inverse temperature\n",
    "run_n = 75\n",
    "\n",
    "print('beta!=0, 1st reward>23 episode=')\n",
    "best_epi_beta = []\n",
    "for run_index in range(run_n):\n",
    "    # qTable=np.zeros((5**maxTime*maxTime,6))\n",
    "    qTable=sparse.dok_matrix((5**maxTime*maxTime, 6))\n",
    "    reward_list, pulse_list, best_pp, reward_real_list = Q_table_ML(env, qTable, nEpisodes, alpha_list, gamma, beta)\n",
    "    best_epi_beta.append(len(reward_list))\n",
    "print('mean of episode=', np.average(best_epi_beta))\n",
    "    \n",
    "# beta = np.zeros(nEpisodes)\n",
    "# print('beta=0, 1st reward>23 episode=')\n",
    "# best_epi_beta0 = []\n",
    "# for run_index  in range(run_n):\n",
    "#     qTable=sparse.dok_matrix((5**maxTime*maxTime, 6))\n",
    "#     reward_list, pulse_list, best_pp, reward_real_list = Q_table_ML(env, qTable, nEpisodes, alpha_list, gamma, beta)\n",
    "#     best_epi_beta0.append(len(reward_list))\n",
    "# print('mean of episode=', np.average(best_epi_beta0))\n",
    "    \n",
    "print('learning time='+ str(time.clock() - start_t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('QLearningVSrs-pw0.csv',np.array([best_epi_beta]).T,fmt=['%.1f'],delimiter=',',header=\"Qlearning\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# avr=[]\n",
    "# temp=0\n",
    "# for i in range(len(reward_real_list)):\n",
    "#     temp += reward_real_list[i]\n",
    "#     avr.append(temp/(i+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.savetxt('QLearning.csv',np.array([np.asarray(reward_real_list),np.asarray(avr)]).T,fmt=['%.7f','%.7f'],delimiter=',',header=\"rew,avrrew\") \n",
    "\n",
    "# fig, ax = plt.subplots(figsize=(5,4),tight_layout=True)\n",
    "\n",
    "# ax.plot(reward_real_list,'+')\n",
    "# ax.plot(avr)\n",
    "\n",
    "\n",
    "# # ax.set_xscale('log')\n",
    "\n",
    "# ax.set(xlabel='Episode', ylabel='Reward',\n",
    "#        title='Jt=8.18e-3')        ### The 12 pulse sequence is compared with the 6 pulse sequence\n",
    "#                                    ### We regard the 6 pulse sequence is T long, the 12 pulse sequence is 2T long \n",
    "\n",
    "# # plt.legend(['Fid_ML12', 'Fid_ML_sym12','Fid_ML6', 'Fid_WHH'], loc='best')\n",
    "\n",
    "# plt.savefig('QLearning.eps', dpi=fig.dpi, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
