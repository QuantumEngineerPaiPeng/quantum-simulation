{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement learning for Hamiltonian engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np, os, time, sys, random\n",
    "from core import mod_neuro_evo as utils_ne\n",
    "from core import mod_utils as utils\n",
    "import gym, torch\n",
    "from core import replay_memory\n",
    "from core import ddpg as ddpg\n",
    "import argparse\n",
    "from scipy.linalg import expm, logm\n",
    "import gym_xy\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from copy import deepcopy as dcp\n",
    "import timeit\n",
    "import os\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "render = False\n",
    "\n",
    "env_tag ='xy-v0'\n",
    "env = gym.make('xy-v0')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Pauli(n):\n",
    "    if n==0:\n",
    "      return np.eye(2)\n",
    "    elif n==1:\n",
    "      return np.array([[0,1],[1,0]])\n",
    "    elif n==2:\n",
    "      return np.array([[0,-1j],[1j,0]])\n",
    "    elif n==3:\n",
    "      return np.array([[1,0],[0,-1]])\n",
    "    else:\n",
    "      raise ValueError('Input must be integer from 0 to 3.')\n",
    "\n",
    "# returns sigma_a^p*sigma_b^q, with a,b = 1,2,3, p,q being position\n",
    "def Kron2body(N_atom,a,b,p,q):\n",
    "    y=1\n",
    "    for i in range(N_atom):\n",
    "        if i==p:\n",
    "            y=np.kron(y,Pauli(a))\n",
    "        elif i==q:\n",
    "            y=np.kron(y,Pauli(b))\n",
    "        else:\n",
    "            y=np.kron(y,np.eye(2))\n",
    "    return y\n",
    "\n",
    "def Hamiltonian(N_atom,bc,cplist,model):\n",
    "    H=np.zeros((2**N_atom,2**N_atom))\n",
    "    for pp in range(len(cplist)):\n",
    "        for p in range(N_atom):\n",
    "            if bc=='p':\n",
    "                q=(p+pp+1)%N_atom\n",
    "            elif bc=='o':\n",
    "                q=p+pp+1\n",
    "                if q>=N_atom:\n",
    "                    continue\n",
    "            H=H+cplist[pp]*(model[0]*Kron2body(N_atom,1,1,p,q)\n",
    "                            +model[1]*Kron2body(N_atom,2,2,p,q)\n",
    "                            +model[2]*Kron2body(N_atom,3,3,p,q))+model[3]*Kron2body(N_atom,3,0,p,q)\n",
    "    if np.max(np.abs(np.imag(H)))<1e-10:                                         #why?\n",
    "        H=np.real(H)\n",
    "    return H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxTime=6\n",
    "nSpin=3\n",
    "pw=1\n",
    "env.setParam(maxTime,nSpin,pw)\n",
    "\n",
    "\n",
    "Aim=np.eye(2**env.nSpin)\n",
    "env.setTarget(Aim)\n",
    "\n",
    "H=Hamiltonian(nSpin,'p',[1],[-0.5,-0.5,1,0])\n",
    "J=8.18e-3\n",
    "t=1\n",
    "env.setU0(expm(-1j*J*H*t))   \n",
    "env.setH0(J*H)                                     #  t=1 is the free evolution time 1us"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Parameters:\n",
    "    def __init__(self):\n",
    "\n",
    "        #Number of Frames to Run\n",
    "        self.num_frames = 5000000\n",
    "\n",
    "        #USE CUDA by NVIDIA GPU\n",
    "        self.is_cuda = False; self.is_memory_cuda = False\n",
    "\n",
    "        #Sunchronization Period\n",
    "        self.synch_period = 10\n",
    "\n",
    "        #DDPG params\n",
    "        self.use_ln = True\n",
    "        self.gamma = 0.99; self.tau = 0.001\n",
    "        self.seed = 7\n",
    "        self.batch_size = 128\n",
    "        self.buffer_size = 1000000\n",
    "        self.frac_frames_train = 1.0\n",
    "        self.use_done_mask = True\n",
    "\n",
    "        ###### NeuroEvolution Params ########\n",
    "        #Num of trials\n",
    "        self.num_evals = 1\n",
    "\n",
    "        #Elitism Rate\n",
    "        self.elite_fraction = 0.2\n",
    "\n",
    "\n",
    "        self.pop_size = 10\n",
    "        self.crossover_prob = 0.1\n",
    "        self.mutation_prob = 0.9\n",
    "\n",
    "        #Save Results\n",
    "        self.state_dim = None; self.action_dim = None #Simply instantiate them here, will be initialized later\n",
    "        self.save_foldername = 'R_ERL/'\n",
    "        if not os.path.exists(self.save_foldername): os.makedirs(self.save_foldername)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, args, env):\n",
    "        self.args = args; self.env = env\n",
    "        self.evolver = utils_ne.SSNE(self.args)\n",
    "        self.best_r=0\n",
    "        self.best_state=[]\n",
    "\n",
    "        #Init population\n",
    "        self.pop = []\n",
    "        for _ in range(args.pop_size):\n",
    "            self.pop.append(ddpg.Actor(args))\n",
    "\n",
    "        #Turn off gradients and put in eval mode\n",
    "        for actor in self.pop: actor.eval()\n",
    "\n",
    "        #Init RL Agent\n",
    "        self.rl_agent = ddpg.DDPG(args)\n",
    "        self.replay_buffer = replay_memory.ReplayMemory(args.buffer_size)\n",
    "        self.ounoise = ddpg.OUNoise(args.action_dim)\n",
    "\n",
    "        #Trackers\n",
    "        self.num_games = 0; self.num_frames = 0; self.gen_frames = None\n",
    "\n",
    "    def add_experience(self, state, action, next_state, reward, done):\n",
    "        reward = utils.to_tensor(np.array([reward])).unsqueeze(0)\n",
    "        if self.args.is_cuda: reward = reward.cuda()\n",
    "        if self.args.use_done_mask:\n",
    "            done = utils.to_tensor(np.array([done]).astype('uint8')).unsqueeze(0)\n",
    "            if self.args.is_cuda: done = done.cuda()\n",
    "        action = utils.to_tensor(action)\n",
    "        if self.args.is_cuda: action = action.cuda()\n",
    "        self.replay_buffer.push(state, action, next_state, reward, done)\n",
    "\n",
    "    def evaluate(self, net, is_render, is_action_noise=False, store_transition=True):\n",
    "        total_reward = 0.0\n",
    "\n",
    "        state = self.env.reset()\n",
    "        state = utils.to_tensor(state).unsqueeze(0)\n",
    "#         state = utils.to_tensor(state)\n",
    "        if self.args.is_cuda: state = state.cuda()\n",
    "        done = False\n",
    "        pp=0\n",
    "\n",
    "        while not done:\n",
    "            if store_transition: self.num_frames += 1; self.gen_frames += 1\n",
    "            if render and is_render: self.env.render()\n",
    "            action = net.forward(state)\n",
    "            action.clamp(-1,1)\n",
    "            action = utils.to_numpy(action.cpu())\n",
    "            if is_action_noise: action += self.ounoise.noise()\n",
    "\n",
    "            next_state, reward, done, info = self.env.step(action.flatten(),pp)  #Simulate one step in environment\n",
    "            next_state = utils.to_tensor(next_state).unsqueeze(0)\n",
    "            if self.args.is_cuda:\n",
    "                next_state = next_state.cuda()\n",
    "            total_reward += reward\n",
    "\n",
    "            if store_transition: self.add_experience(state, action, next_state, reward, done)\n",
    "            state = next_state\n",
    "            pp=pp+1\n",
    "        if store_transition: self.num_games += 1\n",
    "            \n",
    "        if total_reward>self.best_r:\n",
    "            self.best_r=total_reward\n",
    "            self.best_state.append(state)\n",
    "            print(state)\n",
    "\n",
    "        return total_reward\n",
    "\n",
    "    def rl_to_evo(self, rl_net, evo_net):\n",
    "        for target_param, param in zip(evo_net.parameters(), rl_net.parameters()):\n",
    "            target_param.data.copy_(param.data)\n",
    "\n",
    "    def train(self):\n",
    "        self.gen_frames = 0\n",
    "\n",
    "        ####################### EVOLUTION #####################\n",
    "        all_fitness = []\n",
    "        #Evaluate genomes/individuals\n",
    "        for net in self.pop:\n",
    "            fitness = 0.0\n",
    "            for eval in range(self.args.num_evals): fitness += self.evaluate(net, is_render=False, is_action_noise=False)\n",
    "            all_fitness.append(fitness/self.args.num_evals)\n",
    "\n",
    "        best_train_fitness = max(all_fitness)\n",
    "        worst_index = all_fitness.index(min(all_fitness))\n",
    "\n",
    "        #Validation test\n",
    "        champ_index = all_fitness.index(max(all_fitness))\n",
    "        test_score = 0.0\n",
    "        for eval in range(5): test_score += self.evaluate(self.pop[champ_index], is_render=True, is_action_noise=False, store_transition=False)/5.0\n",
    "\n",
    "        #NeuroEvolution's probabilistic selection and recombination step\n",
    "        elite_index = self.evolver.epoch(self.pop, all_fitness)\n",
    "\n",
    "\n",
    "        ####################### DDPG #########################\n",
    "        #DDPG Experience Collection\n",
    "        self.evaluate(self.rl_agent.actor, is_render=False, is_action_noise=True) #Train\n",
    "\n",
    "        #DDPG learning step\n",
    "        if len(self.replay_buffer) > self.args.batch_size * 5:\n",
    "            for _ in range(int(self.gen_frames*self.args.frac_frames_train)):\n",
    "                transitions = self.replay_buffer.sample(self.args.batch_size)\n",
    "                batch = replay_memory.Transition(*zip(*transitions))\n",
    "                self.rl_agent.update_parameters(batch)\n",
    "\n",
    "            #Synch RL Agent to NE\n",
    "            if self.num_games % self.args.synch_period == 0:\n",
    "                self.rl_to_evo(self.rl_agent.actor, self.pop[worst_index])\n",
    "                self.evolver.rl_policy = worst_index\n",
    "                print('Synch from RL --> Nevo')\n",
    "\n",
    "        return best_train_fitness, test_score, elite_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## run RL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running xy-v0  State_dim: 25  Action_dim: 10\n",
      "tensor([[ 0.7500,  0.7500,  0.7500,  0.7500,  0.0000,  0.7500,  0.0000,  0.7500,\n",
      "          0.0000,  0.0000,  0.7500,  0.0000,  0.9931,  0.9886,  0.9569,  0.9880,\n",
      "         -0.5097,  0.9780, -0.3195,  0.9614, -0.4250, -0.4701, -0.9452,  0.4092,\n",
      "          1.0000]])\n",
      "#Games: 11 #Frames: 132  Epoch_Max: 9.13  Test_Score: 9.13  Avg: 0.00 ENV xy-v0\n",
      "RL Selection Rate: Elite/Selected/Discarded 0.00 0.00 0.00\n",
      "\n",
      "#Games: 22 #Frames: 264  Epoch_Max: 9.13  Test_Score: 9.13  Avg: 9.13 ENV xy-v0\n",
      "RL Selection Rate: Elite/Selected/Discarded 0.00 0.00 0.00\n",
      "\n",
      "tensor([[ 0.7500,  0.7500,  0.2500,  0.2500,  0.2500,  0.2500,  0.2500,  0.2500,\n",
      "          0.2500,  0.2500,  0.2500,  0.2500, -0.1592, -0.4335, -0.9815, -0.9890,\n",
      "         -0.9907, -0.9903, -0.9862, -0.9922, -0.9918, -0.9905, -0.9904, -0.9903,\n",
      "          1.0000]])\n",
      "#Games: 33 #Frames: 396  Epoch_Max: 9.64  Test_Score: 9.64  Avg: 9.13 ENV xy-v0\n",
      "RL Selection Rate: Elite/Selected/Discarded 0.00 0.00 0.00\n",
      "\n",
      "#Games: 44 #Frames: 528  Epoch_Max: 9.64  Test_Score: 9.64  Avg: 9.30 ENV xy-v0\n",
      "RL Selection Rate: Elite/Selected/Discarded 0.00 0.00 0.00\n",
      "\n",
      "tensor([[ 0.0000,  0.0000,  0.7500,  0.0000,  0.0000,  0.0000,  0.0000,  0.2500,\n",
      "          0.2500,  0.2500,  0.2500,  0.2500, -0.7493, -0.8057,  0.4697, -0.8193,\n",
      "         -0.8057, -0.6963, -0.8437, -0.9713, -0.9546, -0.9457, -0.9454, -0.8846,\n",
      "          1.0000]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xiaoyanghuang/Desktop/QEngineering/cluster/test/E-DDPG-fw-12-1-gradinv-checkpoint/core/ddpg.py:152: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  state_batch.volatile = False; next_state_batch.volatile = True; action_batch.volatile = False\n",
      "/Users/xiaoyanghuang/Desktop/QEngineering/cluster/test/E-DDPG-fw-12-1-gradinv-checkpoint/core/ddpg.py:173: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.critic.parameters(), 10)\n",
      "/Users/xiaoyanghuang/Desktop/QEngineering/cluster/test/E-DDPG-fw-12-1-gradinv-checkpoint/core/ddpg.py:187: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.critic.parameters(), 10)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-8c8f915edb58>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_frames\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_frames\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m     \u001b[0mbest_train_fitness\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merl_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0melite_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mbest_train_fitness\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0mbest_fid\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0mbest_fid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbest_train_fitness\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-bf129647cfae>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    104\u001b[0m                 \u001b[0mtransitions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplay_buffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m                 \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreplay_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTransition\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mtransitions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrl_agent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m             \u001b[0;31m#Synch RL Agent to NE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/QEngineering/cluster/test/E-DDPG-fw-12-1-gradinv-checkpoint/core/ddpg.py\u001b[0m in \u001b[0;36mupdate_parameters\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    170\u001b[0m         \u001b[0mcurrent_q\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcritic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0maction_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m         \u001b[0mdt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent_q\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_q\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m         \u001b[0mdt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m         \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcritic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcritic_optim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    100\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \"\"\"\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "    start = timeit.default_timer()\n",
    "    average_score=[]\n",
    "    frame=[]\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    best_fid=0\n",
    "    best_state=None\n",
    "    parameters = Parameters()  # Create the Parameters class\n",
    "    tracker = utils.Tracker(parameters, ['erl'], '_score.csv')  # Initiate tracker\n",
    "    frame_tracker = utils.Tracker(parameters, ['frame_erl'], '_score.csv')  # Initiate tracker\n",
    "    time_tracker = utils.Tracker(parameters, ['time_erl'], '_score.csv')\n",
    "\n",
    "    #Create Env\n",
    "    parameters.action_dim = env.action_space.shape[0]\n",
    "    parameters.state_dim = env.observation_space.shape[0]\n",
    "\n",
    "    #Seed\n",
    "    env.seed(parameters.seed);\n",
    "    torch.manual_seed(parameters.seed); np.random.seed(parameters.seed); random.seed(parameters.seed)\n",
    "\n",
    "    #Create Agent\n",
    "    agent = Agent(parameters, env)\n",
    "    print('Running', env_tag, ' State_dim:', parameters.state_dim, ' Action_dim:', parameters.action_dim)\n",
    "\n",
    "    next_save = 100; time_start = time.time()\n",
    "    \n",
    "\n",
    "    agent.pop[0].load_state_dict(torch.load(parameters.save_foldername + 'evo_net'))\n",
    "    agent.pop[0].eval()\n",
    "    while agent.num_frames <= parameters.num_frames:\n",
    "        best_train_fitness, erl_score, elite_index = agent.train()\n",
    "        if best_train_fitness>best_fid:\n",
    "            best_fid=best_train_fitness\n",
    "        average_score.append(tracker.all_tracker[0][1])\n",
    "        frame.append(agent.num_frames)\n",
    "        print('#Games:', agent.num_games, '#Frames:', agent.num_frames, ' Epoch_Max:', '%.2f'%best_train_fitness if best_train_fitness != None else None, ' Test_Score:','%.2f'%erl_score if erl_score != None else None, ' Avg:','%.2f'%tracker.all_tracker[0][1], 'ENV '+env_tag)\n",
    "        print('RL Selection Rate: Elite/Selected/Discarded', '%.2f'%(agent.evolver.selection_stats['elite']/agent.evolver.selection_stats['total']),\n",
    "                                                             '%.2f' % (agent.evolver.selection_stats['selected'] / agent.evolver.selection_stats['total']),\n",
    "                                                              '%.2f' % (agent.evolver.selection_stats['discarded'] / agent.evolver.selection_stats['total']))\n",
    "        print()\n",
    "        tracker.update([erl_score], agent.num_games)\n",
    "        frame_tracker.update([erl_score], agent.num_frames)\n",
    "        time_tracker.update([erl_score], time.time()-time_start)\n",
    "\n",
    "        #Save Policy\n",
    "        if agent.num_games > next_save:\n",
    "            next_save += 100\n",
    "            if elite_index != None: torch.save(agent.pop[elite_index].state_dict(), parameters.save_foldername + 'evo_net')\n",
    "            print(\"Progress Saved\")\n",
    "            \n",
    "            \n",
    "    stop = timeit.default_timer()\n",
    "\n",
    "    print('Time: ', stop - start)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save learning process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_score[0]=average_score[1]\n",
    "\n",
    "np.savetxt('ES_DDPG_pw_12_angleshift_period5_gamma0.99_taue-3_seed7_batchsize128_buffersizee6_fracframestrain1_numevals1_elitefraction0.2_popsize10_crossoverprob0.9_mutationprob0.9.csv',np.array([frame,  average_score]).T,fmt=['%.7f','%.7f'],delimiter=',',header=\"co,rew\") \n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5,4),tight_layout=True)\n",
    "\n",
    "ax.plot(frame,  average_score)\n",
    "\n",
    "\n",
    "# ax.set_xscale('log')\n",
    "\n",
    "ax.set(xlabel='Frame', ylabel='avrReward',\n",
    "       title='Jt=8.18e-3')        ### The 12 pulse sequence is compared with the 6 pulse sequence\n",
    "                                   ### We regard the 6 pulse sequence is T long, the 12 pulse sequence is 2T long \n",
    "\n",
    "# plt.legend(['Fid_ML12', 'Fid_ML_sym12','Fid_ML6', 'Fid_WHH'], loc='best')\n",
    "\n",
    "plt.savefig('ES_DDPG_pw_12_angleshift_period5_gamma0.99_taue-3_seed7_batchsize128_buffersizee6_fracframestrain1_numevals1_elitefraction0.2_popsize10_crossoverprob0.9_mutationprob0.9.eps', dpi=fig.dpi, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# temp=torch.cat((agent.best_state[len(agent.best_state)-2][0] ,[agent.best_r]), 0)\n",
    "temp=agent.best_state[len(agent.best_state)-1][0].numpy()\n",
    "temp=np.append(temp,agent.best_r)\n",
    "\n",
    "np.savetxt('ES_DDPG_pw_12_angleshift.csv',np.array(temp).T,fmt=['%.7f'],delimiter=',',header=\"rew\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
